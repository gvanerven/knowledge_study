{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import cv2\n",
    "#from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5012\n",
      "5019\n",
      "cat-and-dog/test_set/cats/cat.4001.jpg\n",
      "cat-and-dog/dogs/dog.1.jpg\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "filepaths = []\n",
    "grp_dataset = {'cats':[], 'dogs':[]}\n",
    "grp_dataset_nobkg = {'cats':[], 'dogs':[]}\n",
    "dir_cats = 'cat-and-dog/training_set/cats'\n",
    "dir_dogs = 'cat-and-dog/dogs'\n",
    "\n",
    "#test_plane = '/kaggle/input/machine-plane/plane_PNG5238.png'\n",
    "#test_cat = '/kaggle/input/catsnobackground/5a5a8b8b14d8c4188e0b08e7.png'\n",
    "#test_dog = '/kaggle/input/dogsnobackground/dog-png-transparent-background-basset-hound-png-file-324.png'\n",
    "\n",
    "for dirname, _, filenames in os.walk('cat-and-dog'):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirname, filename)\n",
    "        aux = filepath.split('/')\n",
    "        if 'dogs' in aux:\n",
    "            grp_dataset['dogs'].append(filepath)\n",
    "        elif 'cats' in aux:\n",
    "            grp_dataset['cats'].append(filepath)\n",
    "#        if 'catsnobackground' in filepath:\n",
    "#            grp_dataset_nobkg['cats'].append(filepath)\n",
    "#        if 'dogsnobackground' in filepath:\n",
    "#            grp_dataset_nobkg['dogs'].append(filepath)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "print(len(grp_dataset['cats']))\n",
    "print(len(grp_dataset['dogs']))\n",
    "print(grp_dataset['cats'][0])\n",
    "print(grp_dataset['dogs'][0])\n",
    "#print(grp_dataset_nobkg['cats'][0])\n",
    "#print(grp_dataset_nobkg['dogs'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(165888, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.drop30 = torch.nn.Dropout(p=0.3)\n",
    "        self.drop70 = torch.nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 165888)\n",
    "        x = self.drop30(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop30(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop70(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MultiLabelMarginLoss()\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#criterion = nn.HingeEmbeddingLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "torch.Size([1024])\n",
      "tensor([True])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "if 'rand_idxs' not in locals():\n",
    "    rand_idxs = []\n",
    "    for x in range(20):\n",
    "      rand_idxs.append(random.randint(0,1023))\n",
    "\n",
    "print(len(rand_idxs))\n",
    "output_zero = torch.zeros((1024), dtype=torch.long)\n",
    "output_cat = torch.zeros((1024), dtype=torch.long)\n",
    "print(output_cat.shape)\n",
    "for i in rand_idxs:\n",
    "    output_cat[i] = 1\n",
    "print(torch.dot(output_cat.T, output_cat) == torch.LongTensor([20]))\n",
    "#print(torch.dot(output_cat.T, output_cat) == torch.FloatTensor([20]))\n",
    "output_zero = output_zero.reshape(1,1024)\n",
    "output_cat = output_cat.reshape(1,1024)\n",
    "print(output_zero.shape)\n",
    "print(output_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_img = 200\n",
    "size_crop = 100\n",
    "#transform = transforms.Compose(\n",
    "#    [transforms.ToTensor(),\n",
    "#     transforms.Pad(20, padding_mode='reflect'),\n",
    "#     transforms.Resize((size_img,size_img)),\n",
    "#     transforms.CenterCrop(size_crop),\n",
    "#     transforms.ToTensor(), \n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "def load_img(path):\n",
    "    return io.imread(path,mode)\n",
    "\n",
    "def sample_rand(ch, sizeH, sizeW):\n",
    "     return torch.rand((ch, sizeH, sizeW)).unsqueeze(0)\n",
    "    \n",
    "def load_imgs(paths):\n",
    "    for path in paths:\n",
    "        yield load_img(path)\n",
    "    \n",
    "def load_data_img(paths):\n",
    "    for path in paths:\n",
    "        yield transform(load_img(path)).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list(lst_imgs):\n",
    "    trains = torch.cat((lst_imgs[0], lst_imgs[1]), 0)\n",
    "    for i in range(2,len(lst_imgs)):\n",
    "        trains = torch.cat((trains, lst_imgs[i]), 0)\n",
    "    return trains\n",
    "    \n",
    "\n",
    "def create_minibatch(lst_imgs=None, ch=3, sH=200, sW=200, label=None, lbRand=None, presetted=None):\n",
    "    if presetted is not None:\n",
    "        trains = cat_list(lst_imgs + presetted)\n",
    "    else:\n",
    "        rnd_lst = []\n",
    "        for i, _ in enumerate(lbRand):\n",
    "            rnd_lst = sample_rand(ch, sH, sW)\n",
    "        trains = cat_list(lst_imgs + rnd_lst)\n",
    "    labels = cat_list(label + lbRand)\n",
    "    return trains, labels\n",
    "\n",
    "\n",
    "#def load_data(lst_files, size_batch):\n",
    "#    for i, _ in range(round(len(lst_files/size_batch))+1):\n",
    "#        lst_files = []\n",
    "#        for load_data_img(paths):\n",
    "    \n",
    "def save_rnds(rnds, path='cat-and-dog/training_set/rnds'):\n",
    "    for i, rnd in enumerate(rnds):\n",
    "        img_rnd = rnd[0]\n",
    "        sps_rnd = img_rnd.shape\n",
    "        filename = os.path.join(path, 'rnd.' + str(i) + '.jpg')\n",
    "        for ch in range(sps_rnd[0]):\n",
    "            img_rnd[ch] = (img_rnd[ch]*254).int()\n",
    "        io.imsave(filename, img_rnd.reshape(sps_rnd[2], sps_rnd[1], sps_rnd[0]).numpy().astype(np.uint8))\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rnd_nocat = []\n",
    "for i in range(int(len(grp_dataset['cats'])*0.7)):\n",
    "    rnd_nocat.append(sample_rand(3, size_img, size_img))\n",
    "    \n",
    "save_rnds(rnd_nocat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5012\n",
      "3508\n"
     ]
    }
   ],
   "source": [
    "labels_cat = [output_cat] * len(grp_dataset['cats'])\n",
    "labels_rnd = [output_zero] * int(len(grp_dataset['cats'])*0.7)\n",
    "\n",
    "print(len(labels_cat))\n",
    "print(len(labels_rnd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])\n",
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "print(torch.dot(labels_cat[1][0].T, output_cat[0]) == torch.LongTensor([20]))\n",
    "print(torch.dot(labels_rnd[1][0].T, output_cat[0]) == torch.LongTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'rnds']\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    data_path = 'cat-and-dog/training_set/'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transform\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader, train_dataset\n",
    "\n",
    "#for batch_idx, (data, target) in enumerate(load_dataset()):\n",
    "trainloader, traindataset = load_dataset()\n",
    "classes = traindataset.classes\n",
    "dict_classes = {'cats': output_cat, 'rnds': output_zero}\n",
    "print(traindataset.classes)\n",
    "print(dict_classes['cats'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_int2vector(lst_int_classes, classes, dict_classes):\n",
    "    lst_aux = []\n",
    "    for int_class in lst_int_classes:\n",
    "        lst_aux.append(dict_classes[classes[int_class]])\n",
    "        \n",
    "    return cat_list(lst_aux)\n",
    "\n",
    "def cats_cint2vec(lst_int_classes):\n",
    "    return class_int2vector(lst_int_classes, classes=classes, dict_classes=dict_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 388 and 375 in dimension 2 at /opt/conda/conda-bld/pytorch_1565287142374/work/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cd82f0303dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 388 and 375 in dimension 2 at /opt/conda/conda-bld/pytorch_1565287142374/work/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        net.to(device)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, int_labels = data\n",
    "        labels = cats_cint2vec(int_labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for epoch in range(0):\n",
    "    running_loss = 0.0\n",
    "    for j, cat in enumerate(load_data_img(grp_dataset_nobkg['cats'])):\n",
    "        #net.to(device)\n",
    "        net\n",
    "        optimizer.zero_grad()\n",
    "        train, labels = create_minibatch(img=cat, label=output_cat, lbRand=output_zero, presetted=rnd_nocat[j%len(rnd_nocat)])\n",
    "        #outputs = net(train.to(device))\n",
    "        outputs = net(train)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        #loss = criterion(outputs, labels.float().to(device))\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        acc += 1\n",
    "        if acc % 1000 == 999:    # print every 10 mini-batches\n",
    "            acc = 0\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "            (epoch + 1, j + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputhat(idxs, output_y, output_yhat):\n",
    "    v_idxs = {}\n",
    "    for i in idxs:\n",
    "        v_idxs[i] = float(output_yhat[0,i])\n",
    "\n",
    "    print(torch.dot(output_y.reshape(1024).T, output_y.reshape(1024)))\n",
    "    print(torch.dot(output_yhat.reshape(1024).T, output_yhat.reshape(1024)))\n",
    "    print(output_yhat[0,idxs[0]-1])\n",
    "    print(output_yhat[0,idxs[0]])\n",
    "    print(v_idxs)\n",
    "    print(output_yhat[0,0:100])\n",
    "    \n",
    "print_outputhat(rand_idxs, output_cat, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_bits(size, idxs, output_yhat, threshold=0.5):\n",
    "    verify = torch.zeros((size))\n",
    "    position = []\n",
    "    print(verify.shape)\n",
    "    for i in range(verify.shape[0]):\n",
    "        if output_yhat[0,i] > threshold:\n",
    "            position.append(i)\n",
    "            verify[i] = 1\n",
    "        else:\n",
    "            verify[i] = 0\n",
    "\n",
    "    idxs.sort()\n",
    "    print(verify)\n",
    "    print(position)\n",
    "    print(idxs)\n",
    "    print(torch.dot(verify.T, verify))\n",
    "    \n",
    "verify_bits(1024, rand_idxs, out, threshold=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = transform(load_img(grp_dataset_nobkg['cats'][1]))\n",
    "test0 = test0.unsqueeze(0)\n",
    "imshow(torchvision.utils.make_grid(test0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t0 = net(test0.to(device))\n",
    "print(out_t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputhat(rand_idxs, output_cat, out_t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_bits(1024, rand_idxs, out_t0, threshold=0.0000014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dot_out1_out2(out1, out2, size):\n",
    "    print(torch.dot(out1.reshape(size).T, out2.reshape(size)))\n",
    "    \n",
    "print_dot_out1_out2(out, out_t0, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = transform(load_img(grp_dataset['cats'][0]))\n",
    "test1 = test1.unsqueeze(0)\n",
    "imshow(torchvision.utils.make_grid(test1))\n",
    "out_t1 = net(test1.to(device))\n",
    "print(out_t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputhat(rand_idxs, output_cat, out_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_bits(1024, rand_idxs, out, threshold=0.0000014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dot_out1_out2(out, out_t0, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl0 = transform(load_img(test_plane))\n",
    "test_pl0 = test_pl0.unsqueeze(0)\n",
    "imshow(torchvision.utils.make_grid(test_pl0))\n",
    "out_plane0 = net(test_pl0.to(device))\n",
    "print(out_plane0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputhat(rand_idxs, output_cat, out_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_bits(1024, rand_idxs, out, threshold=0.0000013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dot_out1_out2(out, out_t0, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tconv1 = nn.Conv2d(3, 6, 5)\n",
    "#tpool = nn.MaxPool2d(2, 2)\n",
    "#tconv2 = nn.Conv2d(6, 16, 5)\n",
    "#tfc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#tfc2 = nn.Linear(120, 84)\n",
    "#tfc3 = nn.Linear(84, 1024)\n",
    "#transforms.Resize((300,400))\n",
    "#resize = transforms.Resize((300,400))\n",
    "\n",
    "for i in range(1):\n",
    "    tx = tpool(F.relu(tconv1(train0)))\n",
    "    print(tx.shape)\n",
    "    tx = tpool(F.relu(tconv2(tx)))\n",
    "    print(tx.shape)\n",
    "    tx = tx.view(-1, 150544)\n",
    "    print(tx.shape)\n",
    "    tx = F.relu(tfc1(tx))\n",
    "    print(tx.shape)\n",
    "    tx = F.relu(tfc2(tx))\n",
    "    print(tx.shape)\n",
    "    tx = torch.sigmoid(tfc3(tx))\n",
    "    print(tx.shape)\n",
    "\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epoch = 1\n",
    "for epoch in range(nr_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, datapath in enumerate(grp_dataset['cats'], 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs, labels = data\n",
    "        img=cv2.imread(datapath,cv2.IMREAD_COLOR)\n",
    "        print(img.shape)\n",
    "        print(transform(img).shape)\n",
    "        trf_img = transform(img).reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "        inputs, labels = trf_img.to(device), output_cat.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
